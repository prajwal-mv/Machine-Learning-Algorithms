{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnWjdvBUYbGxpIJ0CHUVZs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajwal-mv/Machine-Learning-Algorithms/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ML ALgorithms##"
      ],
      "metadata": {
        "id": "yXOjJSIXEq__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MachineLearning is any process by which a system improves \n",
        "performance from experience\n",
        "Machine Learning is concerned with computer \n",
        "programs that automatically improve their \n",
        "performance through experience\n"
      ],
      "metadata": {
        "id": "3_6kAnZQErC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised ML: ##\n",
        "\n",
        "\n",
        "In Supervised learning, models are trained using labelled dataset, where the model learns about each type of data and on basis of that data, machines predict the output.The aim of a supervised learning algorithm is to find a mapping function to map the input variable(x) with the output variable(y)."
      ],
      "metadata": {
        "id": "EjJXn-7wErLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear regression:**\n",
        "\n",
        "Linear regression is one of the most basic types of regression in supervised machine learning. The linear regression model consists of a predictor variable and a dependent variable related linearly to each other where We try to find the relationship between independent variable (input) and a corresponding dependent variable (output).Here the output variables are continous \n",
        "\n",
        "This can be expressed in the form of a straight line y = mx+b\n",
        "\n",
        "m- co-efficients, x - variables,b - intercept\n",
        "\n",
        "The goal of linear regression is to create a best fit line such that the error between predicted and observed values are minimum there varience is same \n",
        "\n",
        "Here the errors are also known as residuals\n",
        "loss function: SSE = (y - pred(y))^2\n",
        "\n",
        "loss function is a method of evaluating how well your machine learning algorithm models your featured data set.\n",
        "\n",
        "Gradient descent is an optimaization algorithim used to find the best optimal minumum m = m-alpha * d/dm\n",
        "minumum b = b-alpha * d/db\n",
        "\n",
        "here it tries to minimize the cost function, the model needs to have the best value of m and m. Initially model selects m and b values randomly and then iteratively update these value in order to minimize the cost function until it reaches the minimum.\n",
        "\n",
        "alpha - is the learning rate we can adjust it to reach global minima with min iteration.\n",
        "\n",
        "cost function , MSE = (y - pred(y))^2 /n\n",
        "\n",
        "cost functions are used to estimate how badly models are performing.simply we can say cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. This is typically expressed as a difference between the predicted value and the actual value by number of features.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "* IV lineat to DV\n",
        "* MSE is close to zero\n",
        "* Homoscedascity or equal varience\n",
        "* No multicolinearity between the Independent variables.\n",
        "* Your data should have no significant outliers. \n",
        "\n",
        "To deal with Heteroscedacity:\n",
        "\n",
        "* log transformation\n",
        "* polynomial fit\n",
        "\n",
        "To deal with multicoliniarty:\n",
        "\n",
        "VIF - varience inflation Factor = 1/1-R2\n",
        "\n",
        "1- not corelated\n",
        "1-5 moderatly corelated\n",
        "5< highly corelated\n",
        "\n",
        "Heatmap Corelation:\n",
        "\n",
        "if its more than 90 and less than 20 we can drop that variable \n",
        "\n",
        "**Evaluation Metrics**\n",
        "\n",
        "1. Mean Squared Error (MSE) : It is the average of the squared difference between the predicted and actual value.MSE penalizes large errors.\n",
        "\n",
        "2. Root Mean Squared Error (RMSE): This is the square root of the average of the squared difference of the predicted and actual value.RMSE penalizes large errors. \n",
        "\n",
        "3. Mean Absolute Error (MAE): the average of the absolute difference between the target value and the value predicted by the model. Not preferred in cases where outliers are prominent.MAE does not penalize large errors.\n",
        "\n",
        "4. R-squared or Coefficient of Determination:We know that residual is the difference between actual and predicted value. Thus, RSS (Residual sum of squares)\n",
        "\n",
        "Consider the case where we don't know the values of the independent variables. We only have the y values. With this, we calculate the mean of the y values. This point can be represented as a horizontal line. Now we calculate the sum of squared error between the mean y value and that of every other y value.\n",
        "\n",
        "The total variation in Y can be given as a sum of squared differences of the distance between every point and the arithmetic mean of Y values. This can be termed as TSS (Total sum of squares).\n",
        "\n",
        "the ratio of the residual error (RSS) against the total error (TSS)  Subtracting that ratio from 1 gives r_squared error\n",
        "\n",
        "5. Adjusted R-squared — selection criterion\n",
        "The main difference between adjusted R-squared and R-square is that R-squared describes the amount of variance of the dependent variable represented by every single independent variable, while adjusted R-squared measures variation explained by only the independent variables that actually affect the dependent variable.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/640/1*stA8fyU3xZI7sMhzNNeiCA.webp)\n",
        "\n",
        "Adjusted R-squared. \n",
        "In the equation above, n is the number of data points while k is the number of variables in your model, excluding the constant.\n",
        "\n",
        "R² tends to increase with an increase in the number of independent variables. This could be misleading. Thus, the adjusted R-squared penalizes the model for adding furthermore independent variables (k in the equation) that do not fit the model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tkaotVqoHfhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization**\n",
        "\n",
        "Sometimes the machine learning model performs well with the training data but does not perform well with the test data. It means the model is not able to predict the output when deals with unseen data by introducing noise in the output, and hence the model is called overfitted. This problem can be deal with the help of a regularization technique.\n",
        "\n",
        "Regularization is a technique to prevent the model from overfitting by adding a penalty to it.\n",
        "\n",
        "There are mainly two types of regularization techniques, which are given below:\n",
        "\n",
        "1. Ridge Regression\n",
        "2. Lasso Regression\n",
        "\n",
        "* Ridge Regression\n",
        "Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so to reduce the complexity of the model. It is also called as L2 regularization.\n",
        "In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n",
        "\n",
        "cost function in ridge regression will be:\n",
        "=  RSS + lambda *sum(Beta^2)\n",
        "\n",
        "* Lasso Regression:\n",
        "Lasso regression is another regularization technique to reduce the complexity of the model. It stands for Least Absolute and Selection Operator.\n",
        "It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\n",
        "Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\n",
        "It is also called as L1 regularization. The equation for the cost function of \n",
        "\n",
        "Lasso regression will be:\n",
        "= RSS + lamba * |Beta|\n",
        "\n",
        "* Some of the features in this technique are completely neglected for model evaluation.\n",
        "Hence, the Lasso regression can help us to reduce the overfitting in the model as well as the feature selection."
      ],
      "metadata": {
        "id": "PitT5wg8oTS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear discriminant analysis (LDA)** is used here to reduce the number of features to a more manageable number before the process of classification. Each of the new dimensions generated is a linear combination of pixel values, which form a template.\n",
        "\n",
        "**Principal Component Analysis** is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation. These new transformed features are called the Principal Components. \n",
        "\n"
      ],
      "metadata": {
        "id": "YGqwqzMDm7VZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outliers** are those data points that are significantly different from the rest of the dataset. They are often abnormal observations that skew the data distribution, and arise due to inconsistent data entry, or erroneous observations\n",
        "\n",
        "\n",
        "**Anomaly detection** is the process of identifying unexpected items or events in data sets, which differ from the norm. And anomaly detection is often applied on unlabeled data which is known as unsupervised anomaly detection.\n",
        "\n",
        "Anomaly detection has two basic assumptions:\n",
        "\n",
        "Anomalies only occur very rarely in the data.\n",
        "Their features differ from the normal instances significantly.\n",
        "\n",
        "**Isolation Forest**\n",
        "Isolation Forest is an algorithm to detect outliers that returns the anomaly score of each sample using the IsolationForest algorithm which is based on the fact that anomalies are data points that are few and different. Isolation Forest is a tree-based model. In these trees, partitions are created by first randomly selecting a feature and then selecting a random split value between the minimum and maximum value of the selected feature."
      ],
      "metadata": {
        "id": "0BknVMjgppYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision trees**\n",
        "\n",
        "A decision tree is a type of supervised learning algorithm that can be\n",
        "used in classification as well as regressor problems. The input to a\n",
        "decision tree can be both continuous as well as categorical. A Decision tree is a flowchart-like tree structure, The decision tree works on an if-then statement where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. \n",
        "\n",
        "* Root Nodes – It is the node present at the beginning of a decision tree from this node the population starts dividing according to various features.\n",
        "* Decision Nodes – the nodes we get after splitting the root nodes are called Decision Node\n",
        "* Leaf Nodes – the nodes where further splitting is not possible are called leaf nodes or terminal nodes\n",
        "* Sub-tree – just like a small portion of a graph is called sub-graph similarly a sub-section of this decision tree is called sub-tree.\n",
        "\n",
        "Pruning – is nothing but cutting down some nodes to stop overfitting.\n",
        "\n",
        "Entropy basically measures the impurity of a node. Entropy varies from 0 to 1. 0 if all the data belong to a single class and\n",
        "1 if the class distribution is equal.\n",
        " In this way, entropy will give a\n",
        "measure of impurity in the dataset. like 6Y/2N is pure.\n",
        "\n",
        "H = -plogp – qlog-q  base-2\n",
        "\n",
        "Its is difference between -ve log base2 of postive class and negative class.\n",
        "\n",
        "Information Gain\n",
        "Information gain measures the reduction of uncertainty given some feature and it is also a deciding factor for which attribute should be selected as a decision node or root node.\n",
        "\n",
        "IG = 1 - Entropy\n",
        "\n",
        "entropy low is best 0 - 1\n",
        "gain high is better\n",
        "\n",
        "Gini Impurity is a measurement used to build Decision Trees to determine how the features of a dataset should split nodes to form the tree. More precisely, the Gini Impurity of a dataset is a number between 0-0.5, which indicates the likelihood of new, random data being misclassified if it were given a random class label according to the class distribution in the dataset.\n",
        "\n",
        "Gini = (p^2 + q^2)\n",
        "\n",
        "Gini Impurity = 1- Gini.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KOQtOl2NscE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Methods##\n"
      ],
      "metadata": {
        "id": "QfpR_9J90aoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ensembles means it is group of predictive models combined together to achive better accuracy and model stability.\n",
        "\n",
        "Homogenous \n",
        "\n",
        "**Bagging**\n",
        "\n",
        "Bagging, also known as Bootstrap aggregating, is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms.It is parallel series. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model. Bagging avoids overfitting of data and is used for both regression and classification models, specifically for decision tree algorithms.\n",
        "\n",
        "Bootstrap Aggregation:\n",
        "\n",
        "Bootstrap means it divides large data sets into multiple n sets with using method of row sampling with replacement. and finally it aggregates using the majority votes in case of classification or taking average or mean in case of Regression.\n",
        "\n",
        "**Boosting**\n",
        "\n",
        "Boosting is an ensemble modeling technique that attempts to build a strong model from the number of weak models. It is done by building a model by using weak models in series.Here in each model errors are being implemented so there will be better accuracy and it aims to decrease bias, not variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "lCUWUXYWvAIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**\n",
        "\n",
        "Random Forest is an ensemble machine learning algorithm that follows\n",
        "the bagging technique.Mainly Bootstrap aggregation here the base estimators in the random forest are decision trees. Random forest randomly(row samplig with replacement) selects a set of features that are used to decide the best split at each node of the decision tree.\n",
        "Looking at it step-by-step, this is what a random forest model does:\n",
        "1. Random subsets are created from the original dataset (bootstrapping).\n",
        "2. At each node in the decision tree, only a random set of features are\n",
        "considered to decide the best split.\n",
        "3. A decision tree model is fitted on each of the subsets.\n",
        "4. The final prediction is calculated by averaging the predictions from\n",
        "\n",
        "\n",
        "Difference Between Decision trees and random forest\n",
        "\n",
        "Decision trees\n",
        "1. Decision trees normally suffer from the problem of overfitting if it’s allowed to grow without any control.\n",
        "2. A single decision tree is faster in computation.\n",
        "3. When a data set with features is taken as input by a decision tree it will formulate some set of rules to do prediction.\n",
        "\n",
        "\n",
        "Random forests\n",
        "1. Random forests are created from subsets of data and the final output is based on average or majority ranking and hence the problem of overfitting is taken care of.\n",
        "2. It is comparatively slower.\n",
        "3. Random forest randomly selects observations, builds a decision tree and the average result is taken. It doesn’t use any set of formulas.\n",
        "\n"
      ],
      "metadata": {
        "id": "-ebEU4kNvAcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Boosting##"
      ],
      "metadata": {
        "id": "jzEs2NE_634F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adaptive boosting** or **AdaBoost**: This method operates iteratively, identifying misclassified data points and adjusting their weights to minimize the training error.With AdaBoost, however, the algorithm only makes a node with two leaves, known as Stump. The model continues optimize in a sequential fashion until it yields the strongest predictor.  "
      ],
      "metadata": {
        "id": "tX6XXzHvtKgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Gradient boosting:**\n",
        "The name, gradient boosting, is used since it combines the gradient descent algorithm and boosting method.Gradient boosting re-defines boosting as a numerical optimisation problem where the objective is to minimise the loss function of the model by adding weak learners using gradient descent. Gradient descent is a first-order iterative optimisation algorithm for finding a local minimum of a differentiable function.It works by sequentially adding predictors to an ensemble with each one correcting for the errors of its predecessor.The gradient boosting trains on the residual errors of the previous predictor to achive better accuarcy\n",
        "\n",
        "here first it creates  a base model using avg of output then it calculates difference between base model and and output and creates pseoudo and it creates multiple decision tree here error is implemented in each residuals and finally we get better accuracy but there is problem of over fitting so we add a alpha learning rate here \n",
        "\n",
        "= h0(x) + alpha1* h1 (x) + ......+ alpha(n)*h(xn)"
      ],
      "metadata": {
        "id": "GbuTN5CyptvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##other algorithms##"
      ],
      "metadata": {
        "id": "ns9seJZVsm_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K Nearest Neighbour** :\n",
        "\n",
        "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning algorithm used for both regression and classfication.\n",
        "\n",
        "Steps in KNN\n",
        "Suppose there are two categories, i.e., Category A and Category B, and we have a new data point x1, so this data point will lie in which of these categories. T\n",
        "\n",
        "Step-1: Select the number K of the neighbors\n",
        "\n",
        "Step-2: Calculate the Euclidean distance of K number of neighbors\n",
        "\n",
        "Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n",
        "\n",
        "Step-4: Among these k neighbors, count the number of the data points in each category.\n",
        "\n",
        "Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n",
        "\n",
        "Step-6: Our model is ready.\n",
        "\n",
        "For classification problems, a class label is assigned on the basis of a majority vote—i.e. the label that is most frequently represented around a given data point is used.\n",
        "\n",
        "Regression problems use a similar concept as classification problem, but in this case, the average the k nearest neighbors is taken to make a prediction about a classification. The main distinction here is that classification is used for discrete values, whereas regression is used with continuous ones. However, before a classification can be made, the distance must be defined. Euclidean distance is most commonly used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOmivxfMWI5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Naive Bayes Classifier**\n",
        "\n",
        "Conditional probability is defined as the likelihood of an event or outcome occurring, based on the occurrence of a previous event or outcome.\n",
        "\n",
        "\n",
        "Bayes’ Theorem finds the probability of an event occurring given the\n",
        "probability of another event that has already occurred. Bayes’ theorem\n",
        "is stated mathematically as the following equation\n",
        "\n",
        ": P(y|X) = {P(X|y) P(y)}/{P(X)}\n",
        "where, y is class variable and X is a independent feature\n",
        "vector (of size n) where: X = (x_1,x_2,x_3,.....,x_n)\n",
        "\n",
        "* P(y) is the probability of hypothesis H being true. This is known as the prior probability.\n",
        "* P(X) is the probability of the evidence(regardless of the hypothesis).\n",
        "* P(X|y) is the probability of the evidence given that hypothesis is true.\n",
        "* P(y|X) is the probability of the hypothesis given that the evidence is there.\n",
        "\n",
        "Naïve Bayes algorithm is a supervised learning Probablistic algorithm, which is based on Bayes theorem and used for solving classification problems.Naive Bayes Classifier assumes all the input variables are indpendent hence the term NAIVE.\n",
        "Here it calculates P(probability) for every observation and which P is higher then it is selected as class for the observation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gnb6G__kxFJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Support Vector Machines**\n",
        "\n",
        "SVM is a supervised machine learning problem where we try to find a hyperplane that best separates the two classes. The main difference is logistic regression is a probabilistic approach whereas support vector machine is based on statistical approaches.\n",
        "\n",
        "SVM is supervised ML model used for both classification and regression where first it creates a margin and margin creates two parallel planes above and below the margin called hyperplanes which passes through each one of the data points in each category and these data points are called support vectors here data points should be linearly separeble and margin distance should be maximum.\n",
        "\n",
        "Types:\n",
        "\n",
        "1. Linear SVM:\n",
        "\n",
        "When the data is perfectly linearly separable only then we can use Linear SVM. Perfectly linearly separable means that the data points can be classified into 2 classes by using a single straight line(if 2D).\n",
        "\n",
        "2. Non-Linear SVM:\n",
        "\n",
        "When the data is not linearly separable then we can use Non-Linear SVM, which means when the data points cannot be separated into 2 classes by using a straight line (if 2D) then we use some advanced techniques like kernel tricks to classify them\n",
        "\n",
        "when some data points lie inside the marginal hyperplanes then we consider it as error.Margin without erroe is called hard margin and with error is called as soft margin.\n",
        "\n"
      ],
      "metadata": {
        "id": "mSjWjr0y8JRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UnSupervised ML##"
      ],
      "metadata": {
        "id": "_AePiP2uY00f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets.The goal of unsupervised learning is to find the underlying structure of dataset, group that data according to similarities, and represent that dataset in a compressed format\n",
        "\n",
        "**Clustering :**\n",
        "It is an unsupervised ML algorithm it deals with finding a structure in a collection of unlabeled data.It is a collection of objects which are “similar” between them and are “dissimilar” to the objects belonging to other clusters.\n",
        "\n",
        "\n",
        "**K - means Clustering:**\n",
        "\n",
        "It is an unsupervised machine learning iterative algorithm that divides the unlabeled dataset into k different clusters in such a way that each dataset belongs only one group that has similar properties.Where k represents number of clusters.\n",
        "Centroid based cluster\n",
        "\n",
        "It uses Expectation–maximization (E–M) technique to create custers\n",
        "In short, the expectation–maximization approach here consists of the following procedure:\n",
        "1.\tGuess some cluster centers\n",
        "2.\tRepeat until converged\n",
        "\n",
        "\tE-Step: assign points to the nearest cluster center\n",
        "\n",
        "\tM-Step: set the cluster centers to the mean\n",
        "\n",
        "\n",
        "**Steps in K means:**\n",
        "\n",
        "Step-1: Select the number K to decide the number of clusters.\n",
        "\n",
        "Step-2: Select random K points or centroids. (It can be other from the input dataset).\n",
        "\n",
        "Step-3: Assign each data point to their closest centroid, which will form the predefined K clusters.\n",
        "\n",
        "Step-4: Calculate the variance and place a new centroid of each cluster.\n",
        "\n",
        "Step-5: Repeat the third steps, which means reassign each datapoint to the new closest centroid of each cluster.\n",
        "\n",
        "Step-6: If any reassignment occurs, then go to step-4 else go to FINISH.\n",
        "\n",
        "Step-7: The model is ready.\n",
        "\n",
        "Finding the optimal number of clusters is an important part of this algorithm. A commonly used method for finding optimal K value is Elbow Method.  \n",
        "\n",
        "**Elbow Method**\n",
        "\n",
        "The elbow method is used to determine the optimal number of clusters in k-means clustering. The elbow method plots the value of the cost function produced by different values of k v/s number of K when there is sudden fall or abrupt that point we consider as optimal number of K.\n",
        "cost function = sum(SSE) = (xi - ci)^2\n",
        "\n",
        "\n",
        "xi - data points,\n",
        "ci - centroid\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6594Ogg-Y4mJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hierarchical clustering**\n",
        "\n",
        "Hierarchical clustering, also known as hierarchical cluster analysis (HCA), is an unsupervised clustering algorithm that can be categorized in two ways; they can be agglomerative or divisive. Agglomerative clustering is considered a “bottoms-up approach.” Its data points are isolated as separate groupings initially, and then they are merged together iteratively on the basis of similarity until one cluster has been achieved.Hierarchical clustering takes away the problem of having to pre-define the number of clusters as in kmeans.\n",
        "\n",
        "Four different methods are commonly used to measure similarity:\n",
        "\n",
        "* Ward’s linkage: This method states that the distance between two clusters is defined by the increase in the sum of squared after the clusters are merged.\n",
        "*Average linkage: This method is defined by the mean distance between two points in each cluster\n",
        "* Complete (or maximum) linkage: This method is defined by the maximum distance between two points in each cluster\n",
        "* Single (or minimum) linkage: This method is defined by the minimum distance between two points in each cluster\n",
        "\n",
        "steps in Hierarchical clustering:\n",
        "\n",
        "Hierarchical clustering employs a measure of distance/similarity to create new clusters. Steps for Agglomerative clustering can be summarized as follows:\n",
        "\n",
        "Step 1: Compute the proximity matrix using a particular distance metric\n",
        "Step 2: Each data point is assigned to a cluster\n",
        "Step 3: Merge the clusters based on a metric for the similarity between clusters\n",
        "Step 4: Update the distance matrix\n",
        "Step 5: Repeat Step 3 and Step 4 until only a single cluster remains\n",
        "\n",
        "\n",
        "Dendograms are the diagram which represents hierarchical clustering.\n",
        "\n"
      ],
      "metadata": {
        "id": "lfNKrLW9eEfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhouette score** \n",
        "is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters. To calculate the Silhouette score for each observation/data point, the following distances need to be found out for each observations belonging to all the clusters:\n",
        "\n",
        "Mean distance between the observation and all other data points in the same cluster. This distance can also be called a mean intra-cluster distance. The mean distance is denoted by a.\n",
        "\n",
        "Mean distance between the observation and all other data points of the next nearest cluster. This distance can also be called a mean nearest-cluster distance. The mean distance is denoted by b.\n",
        "\n",
        "The Silhouette Coefficient for a sample is  S=(b−a)/max(a,b)\n",
        "\n",
        "max(a,b) = (b/a) -1.\n",
        "\n",
        "it varies from 1 to -1"
      ],
      "metadata": {
        "id": "zCKQ1ENVgy7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DBSCAN Clustering**\n",
        "\n",
        "DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise.\n",
        "desity based cluster\n",
        "DBSCAN is a clustering algorithm that defines clusters as continuous regions of high density and works well if all the clusters are dense enough and well separated by low-density regions.\n",
        "\n",
        "In the case of DBSCAN, instead of guessing the number of clusters, will define two hyperparameters: epsilon and minPoints to arrive at clusters.\n",
        "\n",
        "1. Epsilon (ε): A distance measure that will be used to locate the points/to check the density in the neighbourhood of any point.\n",
        "2. minPoints(n): The minimum number of points (a threshold) clustered together for a region to be considered dense.\n",
        "\n",
        "* Core Point(x): Data point that has at least minPoints (n) within epsilon (ε) distance.\n",
        "* Border Point(y): Data point that has at least one core point within epsilon (ε) distance and lower than minPoints (n) within epsilon (ε) distance from it.\n",
        "* Noise Point(z): Data point that has no core points within epsilon (ε) distance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8JXXLZcjGpxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Recommender Systems##"
      ],
      "metadata": {
        "id": "NKtBmoEqIRlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Collaborative Filtering**:\n",
        "This method makes automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on a set of items, A is more likely to have B's opinion for a given item than that of a randomly chosen person.\n",
        "\n",
        "**Content-Based Filtering**\n",
        "This method uses only information about the description and attributes of the items users has previously consumed to model user's preferences. In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present). In particular, various candidate items are compared with items previously rated by the user and the best-matching items are recommended.\n",
        "\n",
        "**Hybrid Approach**\n",
        "Recent research has demonstrated that a hybrid approach, combining collaborative filtering and content-based filtering could be more effective than pure approaches in some cases. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem.\n"
      ],
      "metadata": {
        "id": "Ih7rHVKdI1SU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NLP##"
      ],
      "metadata": {
        "id": "CGhGY6AXot4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic modelling** is an unsupervised machine learning technique that’s capable of scanning a set of documents, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents.\n",
        "\n",
        "\n",
        "**Latent Dirichlet Allocation (LDA)** is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n",
        "\n",
        "**Bag of Words** is a commonly used model that depends on word frequencies or occurrences to train a classifier. This model creates an occurrence matrix for documents or sentence irrespective of its grammatical structure or word order.\n",
        "\n",
        "**Word2Vec** embeds words in a lower-dimensional vector space using a shallow neural network.The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. \n"
      ],
      "metadata": {
        "id": "muS4Gm0hoysT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZZx16SGFNbtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P0NcaRH0IWmi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}