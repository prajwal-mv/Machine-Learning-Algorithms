{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtQGjPZWkhjOk6ODgaU7Qx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajwal-mv/Machine-Learning-Algorithms/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ML ALgorithms##"
      ],
      "metadata": {
        "id": "yXOjJSIXEq__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MachineLearning is any process by which a system improves \n",
        "performance from experience\n",
        "Machine Learning is concerned with computer \n",
        "programs that automatically improve their \n",
        "performance through experience\n"
      ],
      "metadata": {
        "id": "3_6kAnZQErC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised ML: ##\n",
        "\n",
        "\n",
        "In Supervised learning, models are trained using labelled dataset, where the model learns about each type of data and on basis of that data, machines predict the output.The aim of a supervised learning algorithm is to find a mapping function to map the input variable(x) with the output variable(y)."
      ],
      "metadata": {
        "id": "EjJXn-7wErLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear regression:**\n",
        "\n",
        "Linear regression is one of the most basic types of regression in supervised machine learning. The linear regression model consists of a predictor variable and a dependent variable related linearly to each other where We try to find the relationship between independent variable (input) and a corresponding dependent variable (output).Here the output variables are continous \n",
        "\n",
        "This can be expressed in the form of a straight line y = mx+b\n",
        "\n",
        "m- co-efficients, x - variables,b - intercept\n",
        "\n",
        "The goal of linear regression is to create a best fit line such that the error between predicted and observed values are minimum there varience is same \n",
        "\n",
        "Here the errors are also known as residuals\n",
        "loss function: SSE = (y - pred(y))^2\n",
        "\n",
        "loss function is a method of evaluating how well your machine learning algorithm models your featured data set.\n",
        "\n",
        "Gradient descent is an optimaization algorithim used to find the best otimal minumum m = m-alpha * d/dm\n",
        "minumum b = b-alpha * d/db\n",
        "\n",
        "here it tries to minimize the cost function, the model needs to have the best value of m and m. Initially model selects m and b values randomly and then iteratively update these value in order to minimize the cost function until it reaches the minimum.\n",
        "\n",
        "alpha - is the learning rate we can adjust it to reach global minima with min iteration.\n",
        "\n",
        "cost function , MSE = (y - pred(y))^2 /n\n",
        "\n",
        "cost functions are used to estimate how badly models are performing.simply we can say cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. This is typically expressed as a difference between the predicted value and the actual value by number of features.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "* IV lineat to DV\n",
        "* MSE is close to zero\n",
        "* Homoscedascity or equal varience\n",
        "* No multicolinearity between the Independent variables.\n",
        "* Your data should have no significant outliers. \n",
        "\n",
        "To deal with Heteroscedacity:\n",
        "\n",
        "* log transformation\n",
        "* polynomial fit\n",
        "\n",
        "To deal with multicoliniarty:\n",
        "\n",
        "VIF - varience inflation Factor = 1/1-R2\n",
        "\n",
        "1- not corelated\n",
        "1-5 moderatly corelated\n",
        "5< highly corelated\n",
        "\n",
        "Heatmap Corelation:\n",
        "\n",
        "if its more than 90 and less than 20 we can drop that variable \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tkaotVqoHfhP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5J0sOdh1HdgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision trees**\n",
        "\n",
        "A decision tree is a type of supervised learning algorithm that can be\n",
        "used in classification as well as regressor problems. The input to a\n",
        "decision tree can be both continuous as well as categorical. A Decision tree is a flowchart-like tree structure, The decision tree works on an if-then statement where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. \n",
        "\n",
        "* Root Nodes – It is the node present at the beginning of a decision tree from this node the population starts dividing according to various features.\n",
        "* Decision Nodes – the nodes we get after splitting the root nodes are called Decision Node\n",
        "* Leaf Nodes – the nodes where further splitting is not possible are called leaf nodes or terminal nodes\n",
        "* Sub-tree – just like a small portion of a graph is called sub-graph similarly a sub-section of this decision tree is called sub-tree.\n",
        "\n",
        "Pruning – is nothing but cutting down some nodes to stop overfitting.\n",
        "\n",
        "Entropy basically measures the impurity of a node. Entropy varies from 0 to 1. 0 if all the data belong to a single class and\n",
        "1 if the class distribution is equal.\n",
        " In this way, entropy will give a\n",
        "measure of impurity in the dataset. like 6Y/2N is pure.\n",
        "\n",
        "H = -plogp – qlog-q  base-2\n",
        "\n",
        "Its is difference between -ve log base2 of postive class and negative class.\n",
        "\n",
        "Information Gain\n",
        "Information gain measures the reduction of uncertainty given some feature and it is also a deciding factor for which attribute should be selected as a decision node or root node.\n",
        "\n",
        "IG = 1 - Entropy\n",
        "\n",
        "entropy low is best 0 - 1\n",
        "gain high is better\n",
        "\n",
        "Gini Impurity is a measurement used to build Decision Trees to determine how the features of a dataset should split nodes to form the tree. More precisely, the Gini Impurity of a dataset is a number between 0-0.5, which indicates the likelihood of new, random data being misclassified if it were given a random class label according to the class distribution in the dataset.\n",
        "\n",
        "Gini = (p^2 + q^2)\n",
        "\n",
        "Gini Impurity = 1- Gini.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KOQtOl2NscE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Methods##\n"
      ],
      "metadata": {
        "id": "QfpR_9J90aoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ensembles means it is group of predictive models combined together to achive better accuracy and model stability.\n",
        "\n",
        "Homogenous \n",
        "\n",
        "**Bagging**\n",
        "\n",
        "Bagging, also known as Bootstrap aggregating, is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms.It is parallel series. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model. Bagging avoids overfitting of data and is used for both regression and classification models, specifically for decision tree algorithms.\n",
        "\n",
        "Bootstrap Aggregation:\n",
        "\n",
        "Bootstrap means it divides large data sets into multiple n sets with using method of row sampling with replacement. and finally it aggregates using the majority votes in case of classification or taking average or mean in case of Regression.\n",
        "\n",
        "**Boosting**\n",
        "\n",
        "Boosting is an ensemble modeling technique that attempts to build a strong model from the number of weak models. It is done by building a model by using weak models in series.Here in each model errors are being implemented so there will be better accuracy and it aims to decrease bias, not variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "lCUWUXYWvAIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**\n",
        "\n",
        "Random Forest is an ensemble machine learning algorithm that follows\n",
        "the bagging technique.Mainly Bootstrap aggregation here the base estimators in the random forest are decision trees. Random forest randomly(row samplig with replacement) selects a set of features that are used to decide the best split at each node of the decision tree.\n",
        "Looking at it step-by-step, this is what a random forest model does:\n",
        "1. Random subsets are created from the original dataset (bootstrapping).\n",
        "2. At each node in the decision tree, only a random set of features are\n",
        "considered to decide the best split.\n",
        "3. A decision tree model is fitted on each of the subsets.\n",
        "4. The final prediction is calculated by averaging the predictions from\n",
        "\n",
        "\n",
        "Difference Between Decision trees and random forest\n",
        "\n",
        "Decision trees\n",
        "1. Decision trees normally suffer from the problem of overfitting if it’s allowed to grow without any control.\n",
        "2. A single decision tree is faster in computation.\n",
        "3. When a data set with features is taken as input by a decision tree it will formulate some set of rules to do prediction.\n",
        "\n",
        "\n",
        "Random forests\n",
        "1. Random forests are created from subsets of data and the final output is based on average or majority ranking and hence the problem of overfitting is taken care of.\n",
        "2. It is comparatively slower.\n",
        "3. Random forest randomly selects observations, builds a decision tree and the average result is taken. It doesn’t use any set of formulas.\n",
        "\n"
      ],
      "metadata": {
        "id": "-ebEU4kNvAcj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4134MRV7vEJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Naive Bayes Classifier**\n",
        "\n",
        "Conditional probability is defined as the likelihood of an event or outcome occurring, based on the occurrence of a previous event or outcome.\n",
        "\n",
        "\n",
        "Bayes’ Theorem finds the probability of an event occurring given the\n",
        "probability of another event that has already occurred. Bayes’ theorem\n",
        "is stated mathematically as the following equation\n",
        "\n",
        ": P(y|X) = {P(X|y) P(y)}/{P(X)}\n",
        "where, y is class variable and X is a independent feature\n",
        "vector (of size n) where: X = (x_1,x_2,x_3,.....,x_n)\n",
        "\n",
        "* P(y) is the probability of hypothesis H being true. This is known as the prior probability.\n",
        "* P(X) is the probability of the evidence(regardless of the hypothesis).\n",
        "* P(X|y) is the probability of the evidence given that hypothesis is true.\n",
        "* P(y|X) is the probability of the hypothesis given that the evidence is there.\n",
        "\n",
        "Naïve Bayes algorithm is a supervised learning Probablistic algorithm, which is based on Bayes theorem and used for solving classification problems.Naive Bayes Classifier assumes all the input variables are indpendent hence the term NAIVE.\n",
        "Here it calculates P(probability) for every observation and which P is higher then it is selected as class for the observation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gnb6G__kxFJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "+\n"
      ],
      "metadata": {
        "id": "W5gFaD2AxyHo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}