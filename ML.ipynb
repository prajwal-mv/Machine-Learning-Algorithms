{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7UXeya1+h1Uzr7jNAOki8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prajwal-mv/Machine-Learning-Algorithms/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ML ALgorithms##"
      ],
      "metadata": {
        "id": "yXOjJSIXEq__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MachineLearning is any process by which a system improves \n",
        "performance from experience\n",
        "Machine Learning is concerned with computer \n",
        "programs that automatically improve their \n",
        "performance through experience\n"
      ],
      "metadata": {
        "id": "3_6kAnZQErC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised ML: ##\n",
        "\n",
        "\n",
        "In Supervised learning, models are trained using labelled dataset, where the model learns about each type of data and on basis of that data, machines predict the output.The aim of a supervised learning algorithm is to find a mapping function to map the input variable(x) with the output variable(y)."
      ],
      "metadata": {
        "id": "EjJXn-7wErLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear regression:**\n",
        "\n",
        "Linear regression is one of the most basic types of regression in supervised machine learning. The linear regression model consists of a predictor variable and a dependent variable related linearly to each other where We try to find the relationship between independent variable (input) and a corresponding dependent variable (output).Here the output variables are continous \n",
        "\n",
        "This can be expressed in the form of a straight line y = mx+b\n",
        "\n",
        "m- co-efficients, x - variables,b - intercept\n",
        "\n",
        "The goal of linear regression is to create a best fit line such that the error between predicted and observed values are minimum there varience is same \n",
        "\n",
        "Here the errors are also known as residuals\n",
        "loss function: SSE = (y - pred(y))^2\n",
        "\n",
        "loss function is a method of evaluating how well your machine learning algorithm models your featured data set.\n",
        "\n",
        "Gradient descent is an optimaization algorithim used to find the best otimal minumum m = m-alpha * d/dm\n",
        "minumum b = b-alpha * d/db\n",
        "\n",
        "here it tries to minimize the cost function, the model needs to have the best value of m and m. Initially model selects m and b values randomly and then iteratively update these value in order to minimize the cost function until it reaches the minimum.\n",
        "\n",
        "alpha - is the learning rate we can adjust it to reach global minima with min iteration.\n",
        "\n",
        "cost function , MSE = (y - pred(y))^2 /n\n",
        "\n",
        "cost functions are used to estimate how badly models are performing.simply we can say cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. This is typically expressed as a difference between the predicted value and the actual value by number of features.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "* IV lineat to DV\n",
        "* MSE is close to zero\n",
        "* Homoscedascity or equal varience\n",
        "* No multicolinearity between the Independent variables.\n",
        "\n",
        "To deal with Heteroscedacity:\n",
        "\n",
        "* log transformation\n",
        "* polynomial fit\n",
        "\n",
        "To deal with multicoliniarty:\n",
        "\n",
        "VIF - varience inflation Factor = 1/1-R2\n",
        "\n",
        "1- not corelated\n",
        "1-5 moderatly corelated\n",
        "5< highly corelated\n",
        "\n",
        "Heatmap Corelation:\n",
        "\n",
        "if its more than 90 and less than 20 we can drop that variable \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tkaotVqoHfhP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5J0sOdh1HdgQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}